{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMW Service Manual Model - Inference Testing\n",
    "\n",
    "This notebook provides systematic testing of the finetuned Llama-3.2-3B model.\n",
    "\n",
    "## What This Notebook Does\n",
    "1. Loads your finetuned model from HuggingFace Hub\n",
    "2. Tests predefined queries across all task types\n",
    "3. Evaluates on validation set with ground truth comparison\n",
    "4. Provides error analysis and accuracy metrics\n",
    "\n",
    "## Requirements\n",
    "- GPU: Optional (CPU inference works, just slower)\n",
    "- Model pushed to HuggingFace Hub\n",
    "- Validation data uploaded to Google Drive (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers peft accelerate bitsandbytes\n",
    "\n",
    "# Import libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "print(\"‚úÖ Packages installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Load Model from HuggingFace Hub\n",
    "\n",
    "**‚ö†Ô∏è Important**: Change `model_id` to your HuggingFace model ID!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS to your model ID\n",
    "model_id = \"your-username/bmw-e30-m3-service-manual\"\n",
    "\n",
    "print(f\"üîÑ Loading model: {model_id}\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(\"‚úÖ Tokenizer loaded\")\n",
    "\n",
    "# Load base model\n",
    "print(\"üîÑ Loading base model (Llama-3.2-3B-Instruct)...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "print(\"‚úÖ Base model loaded\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "print(f\"üîÑ Loading LoRA adapter from {model_id}...\")\n",
    "model = PeftModel.from_pretrained(base_model, model_id)\n",
    "print(\"‚úÖ LoRA adapter loaded\")\n",
    "\n",
    "# Set to eval mode\n",
    "model.eval()\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully and ready for inference!\")\n",
    "print(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 150,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    verbose: bool = False\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate response for a given prompt.\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input prompt with task prefix (e.g., \"[SPEC] What is...\")\n",
    "        max_new_tokens: Maximum tokens to generate\n",
    "        temperature: Sampling temperature (0.1=factual, 0.7=balanced, 1.0=creative)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        verbose: Print generation details\n",
    "    \n",
    "    Returns:\n",
    "        Generated response text\n",
    "    \"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Full prompt:\\n{text}\\n\")\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"assistant\" in response:\n",
    "        response = response.split(\"assistant\")[-1].strip()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Generation time: {elapsed:.2f}s\")\n",
    "        print(f\"Tokens generated: {len(outputs[0]) - len(inputs['input_ids'][0])}\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")\n",
    "\n",
    "# Quick test\n",
    "print(\"\\nüß™ Quick test:\")\n",
    "test_response = generate_response(\"[SPEC] What is the torque?\", max_new_tokens=50, verbose=False)\n",
    "print(f\"Response: {test_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Validation Set (Optional)\n",
    "\n",
    "Upload `data/val.jsonl` to Google Drive at `/MyDrive/llm3/data/val.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load validation data\n",
    "val_path = '/content/drive/MyDrive/llm3/data/val.jsonl'\n",
    "\n",
    "try:\n",
    "    val_data = []\n",
    "    with open(val_path) as f:\n",
    "        for line in f:\n",
    "            val_data.append(json.loads(line))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(val_data)} validation examples\")\n",
    "    \n",
    "    # Show distribution\n",
    "    from collections import Counter\n",
    "    task_counts = Counter(ex['meta']['task'] for ex in val_data)\n",
    "    print(\"\\nüìä Validation set distribution:\")\n",
    "    for task, count in sorted(task_counts.items()):\n",
    "        print(f\"  {task:<20} {count:>4}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Validation data not found. Upload val.jsonl to Google Drive.\")\n",
    "    print(\"   You can still run manual tests in the next cells.\")\n",
    "    val_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Predefined Test Cases by Task Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test cases from each task type\n",
    "test_cases = [\n",
    "    {\n",
    "        \"task\": \"SPEC\",\n",
    "        \"query\": \"What is the torque for cylinder head bolts?\",\n",
    "        \"expected\": \"Should return value with unit (e.g., '45 Nm')\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"SPEC\",\n",
    "        \"query\": \"What is the engine displacement?\",\n",
    "        \"expected\": \"Should return value with unit (e.g., '2.3 L')\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"PROCEDURE\",\n",
    "        \"query\": \"How do you adjust valve clearance?\",\n",
    "        \"expected\": \"Should return numbered steps (1. 2. 3...)\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"PROCEDURE\",\n",
    "        \"query\": \"How do you bleed the brake system?\",\n",
    "        \"expected\": \"Should return numbered steps\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"EXPLANATION\",\n",
    "        \"query\": \"Explain the Motronic control unit operation\",\n",
    "        \"expected\": \"Should give technical explanation with details\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"EXPLANATION\",\n",
    "        \"query\": \"Explain the fuel injection system\",\n",
    "        \"expected\": \"Should describe system operation\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"WIRING\",\n",
    "        \"query\": \"What are the wiring details for terminal 15u routing?\",\n",
    "        \"expected\": \"Should describe wire routing/connections\"\n",
    "    },\n",
    "    {\n",
    "        \"task\": \"SPEC\",\n",
    "        \"query\": \"What is the oil capacity?\",\n",
    "        \"expected\": \"Should return value with unit (e.g., '5.0 L')\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run tests\n",
    "print(\"üß™ Running predefined test cases:\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i, test in enumerate(test_cases, 1):\n",
    "    prompt = f\"[{test['task']}] {test['query']}\"\n",
    "    \n",
    "    print(f\"\\n{i}. Task: {test['task']}\")\n",
    "    print(f\"   Query: {test['query']}\")\n",
    "    print(f\"   Expected: {test['expected']}\")\n",
    "    \n",
    "    response = generate_response(prompt, max_new_tokens=150, temperature=0.7)\n",
    "    \n",
    "    print(f\"   Response: {response}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\n‚úÖ Test cases complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Validation Set Evaluation (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a sample of validation data\n",
    "import random\n",
    "\n",
    "if len(val_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No validation data loaded. Upload val.jsonl to run this test.\")\n",
    "else:\n",
    "    sample_size = 10\n",
    "    sample_indices = random.sample(range(len(val_data)), min(sample_size, len(val_data)))\n",
    "    \n",
    "    print(f\"üìä Evaluating on {sample_size} random validation examples:\\n\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        example = val_data[idx]\n",
    "        task = example['meta']['task']\n",
    "        instruction = example['instruction']\n",
    "        ground_truth = example['output']\n",
    "        \n",
    "        prompt = f\"[{task.upper()}] {instruction}\"\n",
    "        response = generate_response(prompt, max_new_tokens=150, temperature=0.7)\n",
    "        \n",
    "        # Simple matching\n",
    "        exact_match = response.strip().lower() == ground_truth.strip().lower()\n",
    "        partial_match = ground_truth.strip().lower() in response.lower()\n",
    "        \n",
    "        print(f\"\\nTask: {task}\")\n",
    "        print(f\"Query: {instruction}\")\n",
    "        print(f\"Ground Truth: {ground_truth}\")\n",
    "        print(f\"Model Output: {response}\")\n",
    "        print(f\"Exact Match: {'‚úÖ' if exact_match else '‚ùå'}\")\n",
    "        print(f\"Partial Match: {'‚úÖ' if partial_match else '‚ùå'}\")\n",
    "        print(\"-\" * 100)\n",
    "    \n",
    "    print(\"\\n‚úÖ Sample evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Full Validation Set Evaluation with Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(val_data) == 0:\n",
    "    print(\"‚ö†Ô∏è No validation data loaded. Upload val.jsonl to run this test.\")\n",
    "else:\n",
    "    # Evaluate on subset (or full set if you have time)\n",
    "    eval_size = min(50, len(val_data))  # Evaluate on first 50 examples\n",
    "    \n",
    "    print(f\"üìà Running full validation evaluation on {eval_size} examples...\\n\")\n",
    "    \n",
    "    results = {\n",
    "        \"exact_match\": 0,\n",
    "        \"partial_match\": 0,\n",
    "        \"no_match\": 0,\n",
    "        \"by_task\": {}\n",
    "    }\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for i, example in enumerate(val_data[:eval_size]):\n",
    "        task = example['meta']['task']\n",
    "        instruction = example['instruction']\n",
    "        ground_truth = example['output']\n",
    "        \n",
    "        prompt = f\"[{task.upper()}] {instruction}\"\n",
    "        response = generate_response(prompt, max_new_tokens=150, temperature=0.7)\n",
    "        \n",
    "        # Matching logic\n",
    "        exact = response.strip().lower() == ground_truth.strip().lower()\n",
    "        partial = ground_truth.strip().lower() in response.lower()\n",
    "        \n",
    "        if exact:\n",
    "            results[\"exact_match\"] += 1\n",
    "            match_type = \"exact\"\n",
    "        elif partial:\n",
    "            results[\"partial_match\"] += 1\n",
    "            match_type = \"partial\"\n",
    "        else:\n",
    "            results[\"no_match\"] += 1\n",
    "            match_type = \"no_match\"\n",
    "            errors.append({\n",
    "                \"task\": task,\n",
    "                \"instruction\": instruction,\n",
    "                \"ground_truth\": ground_truth,\n",
    "                \"response\": response\n",
    "            })\n",
    "        \n",
    "        # Track by task\n",
    "        if task not in results[\"by_task\"]:\n",
    "            results[\"by_task\"][task] = {\"exact\": 0, \"partial\": 0, \"no_match\": 0, \"total\": 0}\n",
    "        results[\"by_task\"][task][match_type] += 1\n",
    "        results[\"by_task\"][task][\"total\"] += 1\n",
    "        \n",
    "        # Progress\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{eval_size}...\")\n",
    "    \n",
    "    # Print results\n",
    "    total = results[\"exact_match\"] + results[\"partial_match\"] + results[\"no_match\"]\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üìä OVERALL RESULTS\")\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Total evaluated: {total}\")\n",
    "    print(f\"Exact matches:   {results['exact_match']} ({results['exact_match']/total*100:.1f}%)\")\n",
    "    print(f\"Partial matches: {results['partial_match']} ({results['partial_match']/total*100:.1f}%)\")\n",
    "    print(f\"No matches:      {results['no_match']} ({results['no_match']/total*100:.1f}%)\")\n",
    "    print(f\"\\nAccuracy (exact + partial): {(results['exact_match'] + results['partial_match'])/total*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"üìä RESULTS BY TASK TYPE\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    for task, stats in sorted(results[\"by_task\"].items()):\n",
    "        total_task = stats[\"total\"]\n",
    "        print(f\"\\n{task.upper()}:\")\n",
    "        print(f\"  Total:   {total_task}\")\n",
    "        print(f\"  Exact:   {stats['exact']} ({stats['exact']/total_task*100:.1f}%)\")\n",
    "        print(f\"  Partial: {stats['partial']} ({stats['partial']/total_task*100:.1f}%)\")\n",
    "        print(f\"  Wrong:   {stats['no_match']} ({stats['no_match']/total_task*100:.1f}%)\")\n",
    "        print(f\"  Accuracy: {(stats['exact'] + stats['partial'])/total_task*100:.1f}%\")\n",
    "    \n",
    "    # Show error examples\n",
    "    if errors:\n",
    "        print(\"\\n\" + \"=\" * 100)\n",
    "        print(f\"‚ùå ERROR EXAMPLES (showing first 5 of {len(errors)})\")\n",
    "        print(\"=\" * 100)\n",
    "        \n",
    "        for i, err in enumerate(errors[:5], 1):\n",
    "            print(f\"\\n{i}. Task: {err['task']}\")\n",
    "            print(f\"   Query: {err['instruction']}\")\n",
    "            print(f\"   Expected: {err['ground_truth']}\")\n",
    "            print(f\"   Got: {err['response']}\")\n",
    "            print(\"-\" * 100)\n",
    "    \n",
    "    print(\"\\n‚úÖ Evaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Interactive Testing\n",
    "\n",
    "Test your own queries interactively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "print(\"üéÆ Interactive Testing Mode\")\n",
    "print(\"Enter your queries below. Format: [TASK] question\")\n",
    "print(\"Tasks: SPEC, PROCEDURE, EXPLANATION, WIRING, TROUBLESHOOTING\")\n",
    "print(\"Type 'quit' to exit\\n\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        query = input(\"\\nYour query: \")\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not query.strip():\n",
    "            continue\n",
    "        \n",
    "        # Check if query has task prefix\n",
    "        if not query.startswith('['):\n",
    "            print(\"‚ö†Ô∏è Query should start with task prefix like [SPEC] or [PROCEDURE]\")\n",
    "            continue\n",
    "        \n",
    "        response = generate_response(query, max_new_tokens=150, temperature=0.7)\n",
    "        print(f\"\\nüí¨ Response: {response}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüëã Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Compare Temperatures\n",
    "\n",
    "Test how different temperature settings affect output quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "test_query = \"[SPEC] What is the torque for cylinder head bolts?\"\n",
    "temperatures = [0.1, 0.5, 0.7, 1.0]\n",
    "\n",
    "print(f\"üå°Ô∏è Testing different temperatures on query: {test_query}\\n\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    response = generate_response(test_query, max_new_tokens=100, temperature=temp)\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "print(\"\\nüí° Lower temperature (0.1-0.3): More deterministic, factual\")\n",
    "print(\"üí° Medium temperature (0.5-0.7): Balanced, good for most cases\")\n",
    "print(\"üí° High temperature (0.8-1.0): More creative, varied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided systematic testing of your finetuned BMW service manual model.\n",
    "\n",
    "### Key Metrics to Track\n",
    "1. **Exact match rate**: How often model outputs match ground truth exactly\n",
    "2. **Partial match rate**: How often model outputs contain correct information\n",
    "3. **Per-task accuracy**: Performance breakdown by task type\n",
    "4. **Error patterns**: Common failure modes\n",
    "\n",
    "### Expected Performance\n",
    "- **SPEC tasks**: Should have highest exact match rate (90%+)\n",
    "- **PROCEDURE tasks**: Should produce numbered steps (80%+)\n",
    "- **EXPLANATION tasks**: Should be coherent and factually accurate (70%+)\n",
    "- **WIRING/TROUBLESHOOTING**: Should provide relevant technical details\n",
    "\n",
    "### Next Steps\n",
    "1. If accuracy < 70%: Consider training longer or with larger LoRA rank\n",
    "2. If specific tasks underperform: Check if training data had enough examples\n",
    "3. If outputs are too random: Lower temperature (0.3-0.5)\n",
    "4. If outputs are too repetitive: Increase temperature slightly (0.7-0.8)\n",
    "\n",
    "### Deployment\n",
    "Once satisfied with performance:\n",
    "1. Create inference API (HuggingFace Inference Endpoints)\n",
    "2. Build demo interface (Gradio/Streamlit)\n",
    "3. Share with BMW enthusiasts/mechanics for feedback\n",
    "4. Iterate based on real-world usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
