{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMW E30 M3 Service Manual - QLoRA Finetuning\n",
    "\n",
    "This notebook finetunes **Llama-3.2-3B-Instruct** on BMW service manual data using QLoRA.\n",
    "\n",
    "## Dataset Statistics\n",
    "- Training: 1,185 examples (balanced across tasks)\n",
    "- Validation: 158 examples\n",
    "- Tasks: SPEC, PROCEDURE, EXPLANATION, WIRING, TROUBLESHOOTING\n",
    "\n",
    "## Requirements\n",
    "- GPU: T4 (Colab free), RTX 3060+, or better\n",
    "- VRAM: ~8-10 GB\n",
    "- Time: ~45-60 minutes on T4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q accelerate peft bitsandbytes transformers trl datasets wandb\n",
    "\n",
    "# Authenticate with HuggingFace\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()  # Enter your HF token when prompted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Mount Google Drive (for dataset upload)\n",
    "\n",
    "**Before running**: Upload the following to Google Drive at `/content/drive/MyDrive/llm3/`:\n",
    "- `config.yaml`\n",
    "- `data/hf_train.jsonl`\n",
    "- `data/hf_val.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Verify files exist\n",
    "import os\n",
    "base_path = '/content/drive/MyDrive/llm3'\n",
    "required_files = [\n",
    "    f'{base_path}/config.yaml',\n",
    "    f'{base_path}/data/hf_train.jsonl',\n",
    "    f'{base_path}/data/hf_val.jsonl'\n",
    "]\n",
    "\n",
    "print(\"Checking required files...\")\n",
    "for file in required_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"\u2705 {file}\")\n",
    "    else:\n",
    "        print(f\"\u274c MISSING: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load Configuration and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load config from Drive\n",
    "config_path = '/content/drive/MyDrive/llm3/config.yaml'\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"\ud83d\udccb Configuration loaded:\")\n",
    "print(f\"  Model: {config['huggingface']['model_name']}\")\n",
    "print(f\"  Epochs: {config['training']['num_epochs']}\")\n",
    "print(f\"  Batch size: {config['training']['per_device_train_batch_size']}\")\n",
    "print(f\"  LoRA rank: {config['qlora']['r']}\")\n",
    "\n",
    "# Load datasets\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return Dataset.from_list(data)\n",
    "\n",
    "train_dataset = load_jsonl('/content/drive/MyDrive/llm3/data/hf_train.jsonl')\n",
    "val_dataset = load_jsonl('/content/drive/MyDrive/llm3/data/hf_val.jsonl')\n",
    "\n",
    "print(f\"\\n\u2705 Loaded datasets - Train: {len(train_dataset)}, Val: {len(val_dataset)}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\ud83d\udcdd Sample training example:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Task: {sample['meta']['task']}\")\n",
    "print(f\"User: {sample['messages'][0]['content']}\")\n",
    "print(f\"Assistant: {sample['messages'][1]['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Load Llama 3.2 3B with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch\n",
    "\n",
    "model_name = config['huggingface']['model_name']\n",
    "print(f\"\ud83d\udd04 Loading model: {model_name}\")\n",
    "\n",
    "# 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load base model in 4-bit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token (Llama doesn't have one by default)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"\u2705 Tokenizer loaded - Vocab size: {len(tokenizer)}\")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model.config.use_cache = False  # Disable cache for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure QLoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=config['qlora']['r'],\n",
    "    lora_alpha=config['qlora']['lora_alpha'],\n",
    "    target_modules=config['qlora']['target_modules'],\n",
    "    lora_dropout=config['qlora']['lora_dropout'],\n",
    "    bias=config['qlora']['bias'],\n",
    "    task_type=config['qlora']['task_type']\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "print(\"\\n\ud83d\udcca Trainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Prepare Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format dataset for SFT (Supervised Fine-Tuning)\n",
    "def format_chat(example):\n",
    "    \"\"\"Apply Llama 3.2 chat template\"\"\"\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        example['messages'], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "print(\"\ud83d\udd04 Formatting datasets with chat template...\")\n",
    "train_dataset = train_dataset.map(format_chat, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(format_chat, remove_columns=val_dataset.column_names)\n",
    "\n",
    "print(f\"\u2705 Formatted datasets ready\")\n",
    "print(f\"\\n\ud83d\udcdd Sample formatted text (first 500 chars):\\n\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./bmw_e30_qlora_results\"\n",
    "\n",
    "# Convert learning_rate to float if it's a string (YAML parsing issue)\n",
    "lr = float(config['training']['learning_rate'])\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=config['training']['num_epochs'],\n",
    "    per_device_train_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "    gradient_checkpointing=True,  # Saves memory\n",
    "    learning_rate=lr,  # Use the converted float\n",
    "    warmup_ratio=config['training']['warmup_ratio'],\n",
    "    weight_decay=config['training']['weight_decay'],\n",
    "    logging_steps=config['training']['logging_steps'],\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    save_strategy=config['training']['save_strategy'],\n",
    "    eval_strategy=\"epoch\",\n",
    "    eval_steps=None,\n",
    "    save_total_limit=2,  # Only keep 2 best checkpoints\n",
    "    fp16=True,  # Mixed precision training\n",
    "    bf16=False,\n",
    "    optim=config['training']['optim'],\n",
    "    report_to=\"none\",  # Change to \"wandb\" if you want tracking\n",
    "    push_to_hub=False,\n",
    "    max_grad_norm=0.3,  # Gradient clipping\n",
    "    lr_scheduler_type=\"cosine\"\n",
    ")\n",
    "\n",
    "effective_batch = training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    "total_steps = (len(train_dataset) // effective_batch) * training_args.num_train_epochs\n",
    "\n",
    "print(\"\u2705 Training arguments configured\")\n",
    "print(f\"\ud83d\udcca Effective batch size: {effective_batch}\")\n",
    "print(f\"\u23f1\ufe0f  Total training steps: ~{total_steps}\")\n",
    "print(f\"\ud83d\udd25 Warmup steps: ~{int(total_steps * training_args.warmup_ratio)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Initialize Trainer and Start Training\n",
    "\n",
    "**Expected training time**:\n",
    "- T4 (Colab free): ~45-60 minutes\n",
    "- A100: ~15-20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# Formatting function to extract text from dataset\n",
    "def formatting_func(example):\n",
    "    return example[\"text\"]\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_func\n",
    ")\n",
    "\n",
    "print(\"\ud83d\ude80 Starting training...\")\n",
    "print(f\"\u23f1\ufe0f  Estimated time: 1-2 hours on T4, 30-45 min on A100\\n\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"\ud83d\udcca Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\ud83d\udcc8 Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Save Model Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "save_dir = \"./bmw_e30_m3_service_manual\"\n",
    "\n",
    "print(f\"\ud83d\udcbe Saving model to {save_dir}...\")\n",
    "trainer.save_model(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"\u2705 Model saved locally\")\n",
    "\n",
    "# Also save to Google Drive for persistence\n",
    "import shutil\n",
    "drive_save_dir = '/content/drive/MyDrive/llm3/models/bmw_e30_m3_service_manual'\n",
    "print(f\"\\n\ud83d\udcbe Copying to Google Drive: {drive_save_dir}...\")\n",
    "shutil.copytree(save_dir, drive_save_dir, dirs_exist_ok=True)\n",
    "print(\"\u2705 Model saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Test Inference (Quick Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "def test_model(prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the assistant's response\n",
    "    response = response.split(\"assistant\\n\")[-1] if \"assistant\" in response else response\n",
    "    return response\n",
    "\n",
    "# Test cases\n",
    "print(\"\ud83e\uddea Testing model with sample queries:\\n\")\n",
    "\n",
    "test_queries = [\n",
    "    \"[SPEC] What is the torque for cylinder head bolts?\",\n",
    "    \"[PROCEDURE] How do you adjust valve clearance?\",\n",
    "    \"[EXPLANATION] Explain the Motronic control unit operation\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\u2753 Query: {query}\")\n",
    "    print(f\"\ud83d\udcac Response: {test_model(query)}\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: Push to HuggingFace Hub (Optional)\n",
    "\n",
    "**Note**: Change `hub_model_id` to your HuggingFace username before running!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push to HuggingFace Hub\n",
    "hub_model_id = \"your-username/bmw-e30-m3-service-manual\"  # \u26a0\ufe0f Change to your username!\n",
    "\n",
    "print(f\"\ud83d\ude80 Pushing model to HuggingFace Hub: {hub_model_id}\")\n",
    "print(\"\u23f1\ufe0f  This may take a few minutes...\\n\")\n",
    "\n",
    "model.push_to_hub(hub_model_id, use_auth_token=True)\n",
    "tokenizer.push_to_hub(hub_model_id, use_auth_token=True)\n",
    "\n",
    "print(f\"\u2705 Model successfully pushed!\")\n",
    "print(f\"\ud83d\udd17 View at: https://huggingface.co/{hub_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Load from Hub (Test Deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading from HuggingFace Hub\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "print(f\"\ud83d\udd04 Loading model from Hub: {hub_model_id}\")\n",
    "\n",
    "# Load config\n",
    "peft_config = PeftConfig.from_pretrained(hub_model_id)\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "model_from_hub = PeftModel.from_pretrained(base_model, hub_model_id)\n",
    "\n",
    "print(\"\u2705 Model loaded from Hub successfully!\")\n",
    "\n",
    "# Quick test\n",
    "test_prompt = \"[SPEC] What is the engine displacement?\"\n",
    "print(f\"\\n\ud83e\uddea Test query: {test_prompt}\")\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(model_from_hub.device)\n",
    "outputs = model_from_hub.generate(**inputs, max_new_tokens=50, temperature=0.7, do_sample=True)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\ud83d\udcac Response: {response.split('assistant')[-1].strip() if 'assistant' in response else response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Evaluate thoroughly**: Test on diverse queries from the validation set\n",
    "2. **Monitor for overfitting**: Check if train/eval loss diverged\n",
    "3. **Adjust hyperparameters** if needed:\n",
    "   - Increase LoRA rank (16 \u2192 32) if underfitting\n",
    "   - Increase dropout (0.05 \u2192 0.1) if overfitting\n",
    "   - Train for more epochs if loss still decreasing\n",
    "4. **Deploy**: Use the model from HuggingFace Hub for inference\n",
    "5. **Collect feedback**: Test with real BMW technicians if possible\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
    "- [Llama 3.2 Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [TRL Documentation](https://huggingface.co/docs/trl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}